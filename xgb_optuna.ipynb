{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e39017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Imports\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    " \n",
    "# -----------------------------\n",
    "# 1. Convert to NumPy\n",
    "# -----------------------------\n",
    "X_train_post_vif_np = X_train_post_vif.values\n",
    "y_train_np = y_train.values\n",
    "X_test_post_vif_np  = X_test_post_vif.values\n",
    "y_test_np  = y_test.values\n",
    " \n",
    "# -----------------------------\n",
    "# 2. Stratified K-Fold\n",
    "# -----------------------------\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    " \n",
    "# -----------------------------\n",
    "# 3. Optuna Objective Function\n",
    "# -----------------------------\n",
    "def objective(trial):\n",
    " \n",
    "    params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": 3,\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"tree_method\": \"hist\",\n",
    " \n",
    "        # ---- Learning control\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.08),\n",
    " \n",
    "        # ---- Structural regularization\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 4),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 10, 50),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.5, 2.0),\n",
    " \n",
    "        # ---- Stochastic regularization\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 0.8),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 0.8),\n",
    " \n",
    "        # ---- Explicit regularization\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.0, 5.0),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1.0, 10.0),\n",
    " \n",
    "        \"nthread\": -1,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    " \n",
    "    fold_losses = []\n",
    "    best_iters  = []   # <<< NEW\n",
    " \n",
    "    for tr_idx, val_idx in skf.split(X_train_post_vif_np, y_train_np):\n",
    " \n",
    "        dtrain = xgb.DMatrix(\n",
    "            X_train_post_vif_np[tr_idx],\n",
    "            label=y_train_np[tr_idx]\n",
    "        )\n",
    "        dval = xgb.DMatrix(\n",
    "            X_train_post_vif_np[val_idx],\n",
    "            label=y_train_np[val_idx]\n",
    "        )\n",
    " \n",
    "        model = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=2000,\n",
    "            evals=[(dval, \"val\")],\n",
    "            early_stopping_rounds=30,\n",
    "            verbose_eval=False\n",
    "        )\n",
    " \n",
    "        # ---- CV metrics\n",
    "        y_val_proba = model.predict(dval)\n",
    "        fold_losses.append(log_loss(y_train_np[val_idx], y_val_proba))\n",
    " \n",
    "        # ---- Capture best iteration\n",
    "        best_iters.append(model.best_iteration)\n",
    " \n",
    "    # ---- Store best iteration for final training\n",
    "    trial.set_user_attr(\"best_iteration\", int(np.mean(best_iters)))\n",
    " \n",
    "    return np.mean(fold_losses)\n",
    " \n",
    "# -----------------------------\n",
    "# 4. Run Optuna Study\n",
    "# -----------------------------\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    " \n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=50,\n",
    "    show_progress_bar=True\n",
    ")\n",
    " \n",
    "# -----------------------------\n",
    "# 5. Best parameters\n",
    "# -----------------------------\n",
    "print(\"\\n✅ Best CV LogLoss:\", study.best_value)\n",
    "print(\"✅ Best Params:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    " \n",
    "# ---- Best number of trees from CV\n",
    "best_n_rounds = study.best_trial.user_attrs[\"best_iteration\"]\n",
    "print(\"✅ Best num_boost_round (CV avg):\", best_n_rounds)\n",
    " \n",
    "# -----------------------------\n",
    "# 6. Train FINAL model on FULL data\n",
    "# -----------------------------\n",
    "dtrain_full = xgb.DMatrix(X_train_post_vif_np, label=y_train_np)\n",
    "dtest = xgb.DMatrix(X_test_post_vif_np)\n",
    " \n",
    "final_params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 3,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"nthread\": -1,\n",
    "    \"random_state\": 42,\n",
    "    **study.best_params\n",
    "}\n",
    " \n",
    "xgb_tuned = xgb.train(\n",
    "    params=final_params,\n",
    "    dtrain=dtrain_full,\n",
    "    num_boost_round=best_n_rounds   # <<< FIX\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
